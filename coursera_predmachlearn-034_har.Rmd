---
title: "coursera_predmachlearn-034_har"
author: "Siyu Sun"
date: "November 22, 2015"
output: html_document
---

## Machine Learning Goal  
  
Classify activity class based on collected activity parameters.  
(Data source:
[Human Activity Recognition - HAR](http://groupware.les.inf.puc-rio.br/har))


```{r setup, echo = T}
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
set.seed(1122)
```


## Load Data  
  
Load training and testing data from `.csv` files.  

* use `read.csv()`  
* na.strings include `"NA"` and `"#DIV/0!"`  
* first 5 columns can be removed since irrelavent to final prediction  
* convert to `dplyr::tbl_df()` more convenient to manipulate  

```{r, echo = T}
training <- read.csv("pml-training.csv", na.strings = c("#DIV/0!", "NA", "na"))[, -(1:5)]
testing <- read.csv("pml-testing.csv", na.strings = c("#DIV/0!", "NA", "na"))[, -(1:5)]
training <- tbl_df(training)
testing <- tbl_df(testing)
by_class <- group_by(training, classe)

```

Dimension of training set is `r nrow(training)` observations x 
`r ncol(training)` variables(features).   

## Data Clean  

Perform data cleaning:  

* Remove feature with `NA` percentage over 90%, relative meaningless to use.
* Check zero covariates in remaining variables, and remove near zero covariate.

  
```{r, echo = T}
per_na <- summarize_each(training, funs(sum(is.na(.))/nrow(training)))
per_na_var <- names(per_na[, per_na > 0.9])
training <- training[, !names(training) %in% per_na_var]
testing <- testing[, !names(testing) %in% per_na_var]
nsv <- nearZeroVar(training, saveMetrics = TRUE)
training <- training[, !names(training) %in% rownames(nsv[nsv$nzv,])]
testing <- testing[, !names(testing) %in% rownames(nsv[nsv$nzv,])]

```


## Preprocess

Perform pre-processing:  

* principle component analysis, down to 20 components
* Standardization (normalization)
* Imputation (however no need in this case)

```{r, echo = T}
### Preprocess ###
# PCA
prComp <- prcomp(training[, -54])
prePCA <- preProcess(training[, -54], method = "pca", pcaComp = 20)
training <- predict(prePCA, training[, -54])
testing <- predict(prePCA, testing[, -54])
# Standardization and no need for imputation
preObj <- preProcess(training[, -54], method = c("center", "scale"))
training <- predict(preObj, training[, -54])
testing <- predict(preObj, testing[, -54])
training$classe <- by_class$classe

```

## Training
  
Perform model training, model selection:  

* suitable for classification (probabily non-linear)
* relative fast (scalable to >10000 obs with `R`)



```{r, echo = T}
inTrain <- createDataPartition(training$classe, p = 0.7, list = F)
trainset <- training[inTrain, ]
testset <- training[-inTrain, ]
# Try different models
# modFit1 <- train(classe ~ ., method = "glm", data = trainset)
 modFit2 <- train(trainset$classe ~ ., method = "lda", data = trainset)
# etc...

# Random Forest
class.rf <- randomForest(x = trainset[, -21], 
                         y = trainset[, 21], 
                         xtest = testset[, -21], 
                         ytest = testset[, 21], 
                         ntree = 100,
                         mtry = 5,
                         do.trace = 5,
                         keep.forest=TRUE)


```


## Evaluation

After a few tests on different models, I decide to use Random Forest after 
seeing the model performance.  

The in sample error and out of sample error are calculated as below.  


```{r, echo = T}
print(class.rf)
insample <- table(class.rf$y, trainset[, 21])
y_testset <- predict(class.rf, newdata = testset[, -21])
outsample <- table(y_testset, testset[, 21])


# In sample error
err_insample <- vector("numeric", 5)
for (i in 1:5)
  err_insample[i] <- sum(insample[i,-i])
err_insample <- sum(err_insample) / sum(insample)

# Out of sample error
err_outsample <- vector("numeric", 5)
for (i in 1:5)
  err_outsample[i] <- sum(outsample[i,-i])
err_outsample <- sum(err_outsample) / sum(outsample)

```

In sample Error:  
`r err_insample`

Out of Sample Error:  
`r err_outsample`

  
  
## Predict
  
Predict the random forest model on the testing data set with 20 observations.  

```{r, echo = T}
prediction <- predict(class.rf, newdata = testing[, -21])

```













